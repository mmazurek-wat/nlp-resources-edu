{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "coursera": {
      "schema_names": [
        "NLPC1-3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmazurek-wat/nlp-en-lab/blob/main/NLP_EN_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcNsom8b3I-S"
      },
      "source": [
        "\n",
        "## 1.0 Predict the Countries from Capitals\n",
        "\n",
        "\n",
        "\n",
        "###  1.1 Importing the data\n",
        "\n",
        "\n",
        "\n",
        "The test dataset for prediction country capitals will be loaded as a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html),\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM3y6TPt3I-Z"
      },
      "source": [
        "# Run this cell to import packages.\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pulling resources necessary to run code\n",
        "# they will be stored in '/content/nlp-resources-edu/05_Embedding_lab/' folder\n",
        "\n",
        "!git clone  https://github.com/mmazurek-wat/nlp-resources-edu.git\n",
        "resource_path = '/content/nlp-resources-edu/05_Embedding_lab/'\n",
        "\n",
        "sys.path.append(resource_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuM_Rv7tnKs3",
        "outputId": "5bade624-f616-4322-d320-9c6db51330a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-resources-edu'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 3), reused 18 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), 3.75 MiB | 6.90 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3BQ5gjx3I-a",
        "outputId": "0a016ab8-e941-43b1-8417-f7ee904d4cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "from os import path\n",
        "capitals_file = 'capitals.txt'\n",
        "capitals_file_path = path.join(resource_path, capitals_file)\n",
        "data = pd.read_csv(capitals_file_path, delimiter=' ')\n",
        "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
        "\n",
        "# print first five elements in the DataFrame\n",
        "data.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    city1 country1    city2     country2\n",
              "0  Athens   Greece  Bangkok     Thailand\n",
              "1  Athens   Greece  Beijing        China\n",
              "2  Athens   Greece   Berlin      Germany\n",
              "3  Athens   Greece     Bern  Switzerland\n",
              "4  Athens   Greece    Cairo        Egypt"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77447ba8-f747-4522-a686-0c337c918354\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city1</th>\n",
              "      <th>country1</th>\n",
              "      <th>city2</th>\n",
              "      <th>country2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Athens</td>\n",
              "      <td>Greece</td>\n",
              "      <td>Bangkok</td>\n",
              "      <td>Thailand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Athens</td>\n",
              "      <td>Greece</td>\n",
              "      <td>Beijing</td>\n",
              "      <td>China</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Athens</td>\n",
              "      <td>Greece</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Athens</td>\n",
              "      <td>Greece</td>\n",
              "      <td>Bern</td>\n",
              "      <td>Switzerland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Athens</td>\n",
              "      <td>Greece</td>\n",
              "      <td>Cairo</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77447ba8-f747-4522-a686-0c337c918354')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77447ba8-f747-4522-a686-0c337c918354 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77447ba8-f747-4522-a686-0c337c918354');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQJmzkHK3I-c"
      },
      "source": [
        "***\n",
        "\n",
        "### To Run This Code On Full Google Dataset:\n",
        "\n",
        "\n",
        "If you want to download the full dataset on your own and choose your own set of word embeddings,\n",
        "please see the instructions and some helper code.\n",
        "\n",
        "- Download the dataset from this [page](https://code.google.com/archive/p/word2vec/).\n",
        "- Search in the page for 'GoogleNews-vectors-negative300.bin.gz' and click the link to download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT8LO9JT7Bn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d509bf4c-bd9b-41be-c269-6f65447db328"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "embeddings = api.load('word2vec-google-news-300')\n",
        "\n",
        "vec_king = embeddings['king']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NILYr-U87J6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb88acd-fae8-43d1-b50d-731c33c8d9bd"
      },
      "source": [
        "\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "#embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
        "\n",
        "#embedding as dictionary\n",
        "\n",
        "\n",
        "word_embeddings = {}\n",
        "for word in embeddings.key_to_index:\n",
        "  word_embeddings[word] = embeddings[word]\n",
        "\n",
        "print(len(word_embeddings))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvVBPGT8w-p"
      },
      "source": [
        "***\n",
        "\n",
        "### To Run On Sample Dataset provided\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "366Btrzb3I-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d72730-1d9f-4532-c809-06660b60e8fd"
      },
      "source": [
        "\n",
        "embedding_subset_file = 'word_embeddings_subset.p'\n",
        "embedding_subset_file_path = path.join(resource_path, embedding_subset_file)\n",
        "\n",
        "word_embeddings = pickle.load(open(embedding_subset_file_path, \"rb\"))\n",
        "len(word_embeddings)  # there should be 243 words that will be used in this assignment"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqPITdO3I-f"
      },
      "source": [
        "Each of the word embedding is a 300-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tognkb_3I-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7153a54d-eb2f-45d5-91d0-9c5381208f22"
      },
      "source": [
        "print(* word_embeddings.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "country city China Iraq oil town Canada London England Australia Japan Pakistan Iran gas happy Russia Afghanistan France Germany Georgia Baghdad village Spain Italy Beijing Jordan Paris Ireland Turkey Egypt Lebanon Taiwan Tokyo Nigeria Vietnam Moscow Greece Indonesia sad Syria Thailand Libya Zimbabwe Cuba Ottawa Tehran Sudan Kenya Philippines Sweden Poland Ukraine Rome Venezuela Switzerland Berlin Bangladesh Portugal Ghana Athens king Madrid Somalia Dublin Qatar Chile Islamabad Bahrain Nepal Norway Serbia Kabul continent Brussels Belgium Uganda petroleum Cairo Denmark Austria Jamaica Georgetown Bangkok Finland Peru Romania Bulgaria Hungary Vienna Kingston Manila Cyprus Azerbaijan Copenhagen Fiji Tunisia Kazakhstan queen Beirut Jakarta Croatia Belarus Algeria Malta Morocco Rwanda Bahamas Damascus Ecuador Angola Canberra Liberia Honduras Tripoli Slovakia Doha Armenia Taipei Oman Nairobi Santiago Guinea Uruguay Stockholm Slovenia Zambia Havana Uzbekistan Belgrade Mogadishu Khartoum Botswana Kyrgyzstan Dhaka Namibia Ankara Abuja Lima Harare Warsaw Malawi Lisbon Latvia Niger Lithuania Estonia Samoa Oslo Nicaragua Hanoi Sofia Macedonia Senegal Mozambique Guyana Mali Accra Kathmandu Tbilisi Helsinki Montenegro Caracas Laos Budapest Kiev Turkmenistan Eritrea Albania Madagascar Nassau Kampala Amman Greenland Belize Moldova Burundi Tajikistan Baku Astana Gambia Bucharest joyful Monrovia Mauritania Algiers Muscat Bern Luanda Dakar Tunis Gabon Minsk Liechtenstein Suva Yerevan Zagreb Bishkek Manama Kigali Riga Lusaka Tashkent Nicosia Valletta Windhoek Dominica Quito Tallinn Bratislava Tegucigalpa Skopje Gaborone Rabat Maputo Suriname Vilnius Montevideo Ljubljana Tirana Dushanbe Ashgabat Asmara Tuvalu Managua Conakry Banjul Bamako Lilongwe Vientiane Chisinau Roseau Nouakchott Podgorica Niamey Bujumbura Apia Antananarivo Libreville Belmopan Vaduz Paramaribo Nuuk Funafuti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wvEA7Sc3I-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7b52e3-61d0-4020-e2aa-a993c648cc1c"
      },
      "source": [
        "print(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimension: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDLw18AN3I-g"
      },
      "source": [
        "### Predict relationships among words\n",
        "\n",
        "Now you will write a function that will use the word embeddings to predict relationships among words.\n",
        "* The function will take as input three words.\n",
        "* The first two are related to each other.\n",
        "* It will predict a 4th word which is related to the third word in a similar manner as the two first words are related to each other.\n",
        "* As an example, \"Athens is to Greece as Bangkok is to ______\"?\n",
        "* You will write a program that is capable of finding the fourth word.\n",
        "* We will give you a hint to show you how to compute this.\n",
        "\n",
        "**pogrubiony tekst**\n",
        "\n",
        "You will implement a function that can tell you the capital of a country.\n",
        "You should use the same methodology shown in the figure above. To do this,\n",
        "compute you'll first compute cosine similarity metric or the Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6JwFqFZ3I-g"
      },
      "source": [
        "### 1.2 Cosine Similarity\n",
        "\n",
        "The cosine similarity function is:\n",
        "\n",
        "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
        "\n",
        "$A$ and $B$ represent the word vectors and $A_i$ or $B_i$ represent index i of that vector.\n",
        "& Note that if A and B are identical, you will get $cos(\\theta) = 1$.\n",
        "* Otherwise, if they are the total opposite, meaning, $A= -B$, then you would get $cos(\\theta) = -1$.\n",
        "* If you get $cos(\\theta) =0$, that means that they are orthogonal (or perpendicular).\n",
        "* Numbers between 0 and 1 indicate a similarity score.\n",
        "* Numbers between -1-0 indicate a dissimilarity score.\n",
        "\n",
        "**Instructions**: Implement a function that takes in two word vectors and computes the cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nackAwsG3I-h"
      },
      "source": [
        "# use numpy.linalg.norm , numpy.dot\n",
        "\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "    '''\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        cos: numerical number representing the cosine similarity between A and B.\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    dot = None\n",
        "    norma = None\n",
        "    normb = None\n",
        "    cos = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return cos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o-h1vgw3I-h",
        "outputId": "d28a5780-528a-4808-f555-46490310810d"
      },
      "source": [
        "# feel free to try different words\n",
        "king = word_embeddings['king']\n",
        "queen = word_embeddings['queen']\n",
        "\n",
        "cosine_similarity(king, queen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLE7gyNY3I-i"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "$\\approx$ 0.6510956"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDKieJ7N3I-j"
      },
      "source": [
        "### 1.3 Euclidean distance\n",
        "\n",
        "You will now implement a function that computes the similarity between two vectors using the Euclidean distance.\n",
        "Euclidean distance is defined as:\n",
        "\n",
        "$$ \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n",
        "\n",
        "* $n$ is the number of elements in the vector\n",
        "* $A$ and $B$ are the corresponding word vectors.\n",
        "* The more similar the words, the more likely the Euclidean distance will be close to 0.\n",
        "\n",
        "**Instructions**: Write a function that computes the Euclidean distance between two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jud9ITLi3I-j"
      },
      "source": [
        "# use numpy.linalg.norm\n",
        "\n",
        "def euclidean(A, B):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        d: numerical number representing the Euclidean distance between A and B.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # euclidean distance\n",
        "\n",
        "    d = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWF5G2h13I-k",
        "outputId": "56955bc6-de17-41a0-ef0b-6a3b34b54fb2"
      },
      "source": [
        "# Test your function\n",
        "euclidean(king, queen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4796925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3U597KF3I-l"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "2.4796925"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05FsJcGh3I-l"
      },
      "source": [
        "### 1.4 Finding the country of each capital\n",
        "\n",
        "Now, you  will use the previous functions to compute similarities between vectors,\n",
        "and use these to find the capital cities of countries. You will write a function that\n",
        "takes in three words, and the embeddings dictionary. Your task is to find the\n",
        "capital cities. For example, given the following words:\n",
        "\n",
        "- 1: Athens 2: Greece 3: Baghdad,\n",
        "\n",
        "your task is to predict the country 4: Iraq.\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. To predict the capital you might want to look at the *King - Man + Woman = Queen* example above, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\n",
        "\n",
        "2. Iterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\n",
        "\n",
        "3. You should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NllY1bpj3I-n"
      },
      "source": [
        "\n",
        "def get_country(city1, country1, city2, embeddings):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        city1: a string (the capital city of country1)\n",
        "        country1: a string (the country of capital1)\n",
        "        city2: a string (the capital city of country2)\n",
        "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
        "    Output:\n",
        "        countries: a dictionary with the most likely country and its similarity score\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "    # store the city1, country 1, and city 2 in a set called group\n",
        "    group = set((city1, country1, city2))\n",
        "\n",
        "    # get embeddings of city 1\n",
        "    city1_emb = None\n",
        "\n",
        "    # get embedding of country 1\n",
        "    country1_emb = None\n",
        "\n",
        "    # get embedding of city 2\n",
        "    city2_emb = None\n",
        "\n",
        "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
        "    # Remember: King - Man + Woman = Queen\n",
        "    vec = None\n",
        "\n",
        "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
        "    similarity = -1\n",
        "\n",
        "    # initialize country to an empty string (you will return it as  a tuple)\n",
        "    country = ''\n",
        "\n",
        "    # loop through all words in the embeddings dictionary\n",
        "    for word in embeddings.keys():\n",
        "\n",
        "        # first check that the word is not already in the 'group' (it is not a part of input dataset)\n",
        "        if word not in group:\n",
        "\n",
        "            # get the word embedding\n",
        "            word_emb = None\n",
        "\n",
        "            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n",
        "            cur_similarity = None\n",
        "\n",
        "            # if the cosine similarity is more similar than the previously best similarity...\n",
        "            if None:\n",
        "\n",
        "                # update the similarity to the new, better similarity\n",
        "                similarity = None\n",
        "\n",
        "                # store the country as a tuple, which contains the word and the similarity\n",
        "                country = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return country"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nqs9TnQ3I-n"
      },
      "source": [
        "# Testing your function, note to make it more robust you can return the 5 most similar words.\n",
        "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YYlQNDE3I-o"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "('Egypt', 0.7626821)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KJznrOwDQQT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekCTL80r3I-o"
      },
      "source": [
        "### 1.5 Model Accuracy\n",
        "\n",
        "Now you will test your new function on the dataset and check the accuracy of the model:\n",
        "\n",
        "$$\\text{Accuracy}=\\frac{\\text{Correct # of predictions}}{\\text{Total # of predictions}}$$\n",
        "\n",
        "**Instructions**: Write a program that can compute the accuracy on the dataset provided for you. You have to iterate over every row to get the corresponding words and feed them into you `get_country` function above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7eSgczi3I-o"
      },
      "source": [
        "\n",
        "def get_accuracy(word_embeddings, data):\n",
        "    '''\n",
        "    Input:\n",
        "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
        "        data: a pandas dataframe containing all the country and capital city pairs\n",
        "\n",
        "    Output:\n",
        "        accuracy: the accuracy of the model\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # initialize num correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through the rows of the dataframe\n",
        "    for i, row in data.iterrows():\n",
        "\n",
        "        # get city1\n",
        "        city1 = None\n",
        "\n",
        "        # get country1\n",
        "        country1 = None\n",
        "\n",
        "        # get city2\n",
        "        city2 =  None\n",
        "\n",
        "        # get country2\n",
        "        country2 = None\n",
        "\n",
        "        # use get_country to find the predicted country2\n",
        "        predicted_country2, _ = None\n",
        "\n",
        "        # if the predicted country2 is the same as the actual country2...\n",
        "        if None:\n",
        "            # increment the number of correct by 1\n",
        "            num_correct += None\n",
        "\n",
        "    # get the number of rows in the data dataframe (length of dataframe)\n",
        "    m = None\n",
        "\n",
        "    # calculate the accuracy by dividing the number correct by m\n",
        "    accuracy = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DulWsieN3I-p"
      },
      "source": [
        "accuracy = get_accuracy(word_embeddings, data)\n",
        "print(f\"Accuracy is {accuracy:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ_Bbyt33I-p"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "$\\approx$ 0.92"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Y4ThGU-Yjn"
      },
      "source": [
        "\n",
        "## 2.0 Get the analogy based on Euclidean distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJNPR1k__0Re"
      },
      "source": [
        "Implement function to find nearest words, which is  closest to requested vector $v$.  (Euclidean distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7U0WxlV-gyk"
      },
      "source": [
        "def find_closest_word(v):\n",
        "  None\n",
        "  return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MDddehzAfXi"
      },
      "source": [
        "Implement function to find analogy like *King - Man + Woman = Queen*  for any word combination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSW9Psf3Ay1e"
      },
      "source": [
        "#rel_src1= Man\n",
        "#rel_trg1= King\n",
        "#rel_src2= Woman\n",
        "#rel_trg2 = ?\n",
        "\n",
        "def get_analogy(rel_src1, rel_trg2, rel_src2):\n",
        "  rel_trg2 = None\n",
        "  return rel_trg2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rfI4OOyBHJJ"
      },
      "source": [
        "Test your functions on  your own subset of words and analogies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFBEBdyBRzm"
      },
      "source": [
        "rel=('fast', 'slow', 'clever')  #fast to slow is like clever to what ?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}